{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMTDOFv2Ms9tAkk47LqpCmc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namkibeom/Korean-ABSA/blob/main/Korean_ABSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "metadata": {
        "id": "vItpu7dptgvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/LorenzoAgnolucci/BERT_for_ABSA.git"
      ],
      "metadata": {
        "id": "1zR7zabltgyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose a dataset and a task { run: \"auto\", display-mode: \"form\" }\n",
        "base_dir = \"/gdrive/MyDrive/Machine_Learning\" #@param {type:\"string\"}\n",
        "dataset_type = \"semeval2014\" #@param [\"sentihood\", \"semeval2014\"]\n",
        "task = \"NLI_M_Kor\" #@param [\"QA_M\", \"NLI_M\", \"QA_B\", \"NLI_B\", \"NLI_M_Kor\"]"
      ],
      "metadata": {
        "id": "QEUt127jtg17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "\n",
        "if dataset_type == \"sentihood\":\n",
        "    id2label = {0: \"None\", 1: \"Positive\", 2: \"Negative\"}\n",
        "    label2id = {\"None\": 0, \"Positive\": 1, \"Negative\": 2}\n",
        "elif dataset_type == \"semeval2014\":\n",
        "    id2label = {0: \"긍정\", 1: \"중립\", 2: \"부정\", 3: \"대립\", 4: \"없음\"}\n",
        "    label2id = {\"긍정\": 0, \"중립\" : 1, \"부정\" : 2, \"대립\": 3, \"없음\": 4}\n",
        "\n",
        "if task.endswith(\"B\"):\n",
        "    num_classes = 2\n",
        "else:\n",
        "    if dataset_type == \"sentihood\":\n",
        "        num_classes = 3\n",
        "    elif dataset_type == \"semeval2014\":\n",
        "        num_classes = 5\n",
        "\n",
        "\n",
        "def get_dataset(path):\n",
        "    original_sentences = []\n",
        "    auxiliary_sentences = []\n",
        "    labels = []\n",
        "    data = pd.read_csv(path, header=0, error_bad_lines= False).values.tolist()\n",
        "    for row in data:\n",
        "        original_sentences.append(row[1])\n",
        "        auxiliary_sentences.append(row[2])\n",
        "        labels.append(row[3])\n",
        "    return original_sentences, auxiliary_sentences, labels\n",
        "\n",
        "\n",
        "\n",
        "train_original_sentences, train_auxiliary_sentences, train_labels = get_dataset(f\"/content/train_NLI_M_Kor.csv\")\n",
        "#if dataset_type == \"sentihood\":\n",
        "    #val_original_sentences, val_auxiliary_sentences, val_labels = get_dataset(f\"{base_dir}/data/{dataset_type}/BERT-pair/dev_{task}.csv\")\n",
        "#elif dataset_type == \"semeval2014\":\n",
        "val_original_sentences, val_auxiliary_sentences, val_labels = get_dataset(f\"/content/test_NLI_M_Kor.csv\")\n",
        "test_original_sentences, test_auxiliary_sentences, test_labels = get_dataset(f\"/content/test_NLI_M_Kor.csv\")\n",
        "\n",
        "test2_original_sentences, test2_auxiliary_sentences, test2_labels = get_dataset(f\"/content/kakao_NLI_M_Kor.csv\")"
      ],
      "metadata": {
        "id": "0PCScqHRtg5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#KR3\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "\n",
        "if dataset_type == \"sentihood\":\n",
        "    id2label = {0: \"None\", 1: \"Positive\", 2: \"Negative\"}\n",
        "    label2id = {\"None\": 0, \"Positive\": 1, \"Negative\": 2}\n",
        "elif dataset_type == \"semeval2014\":\n",
        "    id2label = {0: \"긍정\", 1: \"중립\", 2: \"부정\", 3: \"대립\", 4: \"없음\"}\n",
        "    label2id = {\"긍정\": 0, \"중립\" : 1, \"부정\" : 2, \"대립\": 3, \"없음\": 4}\n",
        "\n",
        "if task.endswith(\"B\"):\n",
        "    num_classes = 2\n",
        "else:\n",
        "    if dataset_type == \"sentihood\":\n",
        "        num_classes = 3\n",
        "    elif dataset_type == \"semeval2014\":\n",
        "        num_classes = 5\n",
        "\n",
        "\n",
        "def get_dataset(path):\n",
        "    original_sentences = []\n",
        "    auxiliary_sentences = []\n",
        "    labels = []\n",
        "    data = pd.read_csv(path, header=0, error_bad_lines= False).values.tolist()\n",
        "    for row in data:\n",
        "        original_sentences.append(row[1])\n",
        "        auxiliary_sentences.append(row[2])\n",
        "        labels.append(row[3])\n",
        "    return original_sentences, auxiliary_sentences, labels\n",
        "\n",
        "\n",
        "\n",
        "train_original_sentences, train_auxiliary_sentences, train_labels = get_dataset(f\"/content/KR3_pair_train.csv\")\n",
        "#if dataset_type == \"sentihood\":\n",
        "    #val_original_sentences, val_auxiliary_sentences, val_labels = get_dataset(f\"{base_dir}/data/{dataset_type}/BERT-pair/dev_{task}.csv\")\n",
        "#elif dataset_type == \"semeval2014\":\n",
        "val_original_sentences, val_auxiliary_sentences, val_labels = get_dataset(f\"/content/KR3_pair_test.csv\")\n",
        "test_original_sentences, test_auxiliary_sentences, test_labels = get_dataset(f\"/content/KR3_pair_test.csv\")"
      ],
      "metadata": {
        "id": "eDKDqDrL7jNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kakao\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "\n",
        "if dataset_type == \"sentihood\":\n",
        "    id2label = {0: \"None\", 1: \"Positive\", 2: \"Negative\"}\n",
        "    label2id = {\"None\": 0, \"Positive\": 1, \"Negative\": 2}\n",
        "elif dataset_type == \"semeval2014\":\n",
        "    id2label = {0: \"긍정\", 1: \"중립\", 2: \"부정\", 3: \"대립\", 4: \"없음\"}\n",
        "    label2id = {\"긍정\": 0, \"중립\" : 1, \"부정\" : 2, \"대립\": 3, \"없음\": 4}\n",
        "\n",
        "if task.endswith(\"B\"):\n",
        "    num_classes = 2\n",
        "else:\n",
        "    if dataset_type == \"sentihood\":\n",
        "        num_classes = 3\n",
        "    elif dataset_type == \"semeval2014\":\n",
        "        num_classes = 5\n",
        "\n",
        "\n",
        "def get_dataset(path):\n",
        "    original_sentences = []\n",
        "    auxiliary_sentences = []\n",
        "    labels = []\n",
        "    data = pd.read_csv(path, header=0, error_bad_lines= False).values.tolist()\n",
        "    for row in data:\n",
        "        original_sentences.append(row[1])\n",
        "        auxiliary_sentences.append(row[2])\n",
        "        labels.append(row[3])\n",
        "    return original_sentences, auxiliary_sentences, labels\n",
        "\n",
        "\n",
        "\n",
        "train_original_sentences, train_auxiliary_sentences, train_labels = get_dataset(f\"/content/train_kakao_NLI_M_Kor.csv\")\n",
        "#if dataset_type == \"sentihood\":\n",
        "    #val_original_sentences, val_auxiliary_sentences, val_labels = get_dataset(f\"{base_dir}/data/{dataset_type}/BERT-pair/dev_{task}.csv\")\n",
        "#elif dataset_type == \"semeval2014\":\n",
        "val_original_sentences, val_auxiliary_sentences, val_labels = get_dataset(f\"/content/test_kakao_NLI_M_Kor.csv\")\n",
        "test_original_sentences, test_auxiliary_sentences, test_labels = get_dataset(f\"/content/test_kakao_NLI_M_Kor.csv\")"
      ],
      "metadata": {
        "id": "f7GbOo1LMKcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# xlm-roberta 주요 라이브러리 설치\n",
        "!pip install mxnet\n",
        "!pip install gluonnlp\n",
        "!pip install sentencepiece\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "_sPrmENjZIh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "yO5qV92bloXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BERT용\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "train_encodings = tokenizer(train_original_sentences, train_auxiliary_sentences, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_original_sentences, val_auxiliary_sentences, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_original_sentences, test_auxiliary_sentences, truncation=True, padding=True)\n",
        "\n",
        "#test2_encodings = tokenizer(test2_original_sentences, test2_auxiliary_sentences, truncation=True, padding=True)"
      ],
      "metadata": {
        "id": "TorhmZ7d3Ymj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_encodings)"
      ],
      "metadata": {
        "id": "1CBK2MbQnsC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Xlm-roberta용\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
        "\n",
        "train_encodings = tokenizer(train_original_sentences, train_auxiliary_sentences, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_original_sentences, val_auxiliary_sentences, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_original_sentences, test_auxiliary_sentences, truncation=True, padding=True)\n",
        "\n",
        "#test2_encodings = tokenizer(test2_original_sentences, test2_auxiliary_sentences, truncation=True, padding=True)"
      ],
      "metadata": {
        "id": "OFQ9oymxtg9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install 'git+https://github.com/SKTBrain/KOBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
      ],
      "metadata": {
        "id": "wKOaEwK9p-d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install torch\n",
        " import torch"
      ],
      "metadata": {
        "id": "QxK8Ew1ushKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kobert\n",
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "\n",
        "#train_encodings = tokenizer(train_original_sentences, train_auxiliary_sentences, truncation=True, padding=True)\n",
        "#val_encodings = tokenizer(val_original_sentences, val_auxiliary_sentences, truncation=True, padding=True)\n",
        "#test_encodings = tokenizer(test_original_sentences, test_auxiliary_sentences, truncation=True, padding=True)\n",
        "\n",
        "#test2_encodings = tokenizer(test2_original_sentences, test2_auxiliary_sentences, truncation=True, padding=True)"
      ],
      "metadata": {
        "id": "VIctDffRpTzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class ABSA_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = ABSA_Dataset(train_encodings, train_labels)\n",
        "val_dataset = ABSA_Dataset(val_encodings, val_labels)\n",
        "test_dataset = ABSA_Dataset(test_encodings, test_labels)\n",
        "\n",
        "#test2_dataset = ABSA_Dataset(test2_encodings, test2_labels)"
      ],
      "metadata": {
        "id": "a1iuH8GlthCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install evaluation"
      ],
      "metadata": {
        "id": "tP39M9HhIWNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "if base_dir not in sys.path:\n",
        "    sys.path.insert(0, f'{base_dir}/')\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import evaluation\n",
        "\n",
        "\n",
        "def get_test_labels(data_dir, dataset_type):\n",
        "    original_sentences = []\n",
        "    auxiliary_sentences = []\n",
        "    labels = []\n",
        "    data = pd.read_csv(f\"{data_dir}/{dataset_type}/BERT-pair/KR3_pair_test.csv\", header=0).values.tolist()\n",
        "    for row in data:\n",
        "        labels.append(row[3])\n",
        "    return labels\n",
        "\n",
        "\n",
        "def get_predictions(data, task, dataset_type):\n",
        "    predicted_labels = []\n",
        "    scores = []\n",
        "    if task.endswith(\"B\"):\n",
        "        if dataset_type == \"sentihood\":\n",
        "            if task.endswith(\"B\"):\n",
        "                count_aspect_rows = 0\n",
        "                current_aspect_scores = []\n",
        "                for row in data:\n",
        "                    current_aspect_scores.append(row[2])\n",
        "                    count_aspect_rows += 1\n",
        "                    if count_aspect_rows % 3 == 0:\n",
        "                        sum_current_aspect_scores = np.sum(current_aspect_scores)\n",
        "                        current_aspect_scores = [score / sum_current_aspect_scores for score in current_aspect_scores]\n",
        "                        scores.append(current_aspect_scores)\n",
        "                        predicted_labels.append(np.argmax(current_aspect_scores))\n",
        "                        current_aspect_scores = []\n",
        "        elif dataset_type == \"semeval2014\":\n",
        "            if task.endswith(\"B\"):\n",
        "                count_aspect_rows = 0\n",
        "                current_aspect_scores = []\n",
        "                for row in data:\n",
        "                    current_aspect_scores.append(row[2])\n",
        "                    count_aspect_rows += 1\n",
        "                    if count_aspect_rows % 5 == 0:\n",
        "                        sum_current_aspect_scores = np.sum(current_aspect_scores)\n",
        "                        current_aspect_scores = [score / sum_current_aspect_scores for score in current_aspect_scores]\n",
        "                        scores.append(current_aspect_scores)\n",
        "                        predicted_labels.append(np.argmax(current_aspect_scores))\n",
        "                        current_aspect_scores = []\n",
        "    return predicted_labels, scores\n",
        "\n",
        "\n",
        "if dataset_type == \"sentihood\":\n",
        "    def compute_metrics(predictions):\n",
        "        scores = [softmax(prediction) for prediction in predictions[0]]\n",
        "        predicted_labels = [np.argmax(x) for x in scores]\n",
        "        if task.endswith(\"B\"):\n",
        "            data = np.insert(scores, 0, predicted_labels, axis=1)\n",
        "            predicted_labels, scores = get_predictions(data, task, dataset_type)\n",
        "        test_labels = get_test_labels(f\"{base_dir}/data\", dataset_type)\n",
        "        metrics = {}\n",
        "        metrics[\"strict_acc\"] = evaluation.compute_sentihood_aspect_strict_accuracy(test_labels, predicted_labels)\n",
        "        metrics[\"F1\"] = evaluation.compute_sentihood_aspect_macro_F1(test_labels, predicted_labels)\n",
        "        metrics[\"aspect_AUC\"] = evaluation.compute_sentihood_aspect_macro_AUC(test_labels, scores)\n",
        "        sentiment_macro_AUC, sentiment_accuracy = evaluation.compute_sentihood_sentiment_classification_metrics(test_labels, scores)\n",
        "        metrics[\"sentiment_acc\"] = sentiment_accuracy\n",
        "        metrics[\"sentiment_AUC\"] = sentiment_macro_AUC\n",
        "        return metrics\n",
        "\n",
        "elif dataset_type == \"semeval2014\":\n",
        "    def compute_metrics(predictions):\n",
        "        scores = [softmax(prediction) for prediction in predictions[0]]\n",
        "        predicted_labels = [np.argmax(x) for x in scores]\n",
        "        if task.endswith(\"B\"):\n",
        "            data = np.insert(scores, 0, predicted_labels, axis=1)\n",
        "            predicted_labels, scores = get_predictions(data, task, dataset_type)\n",
        "        test_labels = get_test_labels(f\"{base_dir}/data\", dataset_type)\n",
        "        metrics = {}\n",
        "        p, r, f1 = evaluation.compute_semeval_PRF(test_labels, predicted_labels)\n",
        "        metrics[\"P\"] = p\n",
        "        metrics[\"R\"] = r\n",
        "        metrics[\"F1\"] = f1\n",
        "        metrics[\"4-way\"] = evaluation.compute_semeval_accuracy(test_labels, predicted_labels, scores, 4)\n",
        "        metrics[\"3-way\"] = evaluation.compute_semeval_accuracy(test_labels, predicted_labels, scores, 3)\n",
        "        metrics[\"binary\"] = evaluation.compute_semeval_accuracy(test_labels, predicted_labels, scores, 2)\n",
        "        return metrics"
      ],
      "metadata": {
        "id": "hoM9bwNW1Cle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluation\n",
        "%run evaluation.py"
      ],
      "metadata": {
        "id": "9enaz3w0NPJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BERT\n",
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments, BertConfig\n",
        "from transformers import logging\n",
        "logging.set_verbosity_debug()\n",
        "\n",
        "\n",
        "epochs = 2\n",
        "batch_size = 3\n",
        "num_steps = len(train_dataset) * epochs // batch_size\n",
        "warmup_steps = num_steps // 10  # 10% of the training steps\n",
        "save_steps = num_steps // epochs    # Save a checkpoint at the end of each epoch\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = f'{base_dir}/models/{dataset_type}/BERT-pair/{task}/',          \n",
        "    num_train_epochs = epochs,              \n",
        "    per_device_train_batch_size = batch_size,  \n",
        "    per_device_eval_batch_size = batch_size,   \n",
        "    warmup_steps = warmup_steps,   \n",
        "    weight_decay = 0.01,               \n",
        "    logging_dir = f'{base_dir}/logs/{dataset_type}/BERT-pair/{task}/',            \n",
        "    logging_steps = 10,\n",
        "    evaluation_strategy = 'epoch',\n",
        "    learning_rate = 2e-5,\n",
        "    save_steps = save_steps\n",
        ")\n",
        "\n",
        "config = BertConfig.from_pretrained(\n",
        "    'bert-base-multilingual-cased',\n",
        "    architectures = ['BertForSequenceClassification'],\n",
        "    hidden_size = 768,\n",
        "    num_hidden_layers = 12,\n",
        "    num_attention_heads = 12,\n",
        "    hidden_dropout_prob = 0.1,\n",
        "    num_labels = num_classes\n",
        ")    \n",
        "\n",
        "#load_finetuned_model = True\n",
        "#if not load_finetuned_model:\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', ignore_mismatched_sizes=True, config=config)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         \n",
        "    args=training_args,                  \n",
        "    train_dataset=train_dataset,         \n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics             \n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(f\"{base_dir}/models/{dataset_type}/BERT-pair/{task}/last_step\")\n",
        "\n",
        "#else:\n",
        "#    model = BertForSequenceClassification.from_pretrained(f\"{base_dir}/models/{dataset_type}/BERT-pair/{task}/last_step\")\n",
        "#\n",
        "#    trainer = Trainer(\n",
        "#        model=model,                         \n",
        "#        args=training_args,                  \n",
        "#        train_dataset=train_dataset,         \n",
        "#        eval_dataset=val_dataset,\n",
        "#        compute_metrics=compute_metrics             \n",
        "#    )"
      ],
      "metadata": {
        "id": "Kgqb1p7wQeLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Xlm-RoBERTa-base\n",
        "from transformers import AutoModelForSequenceClassification, AutoConfig, Trainer, TrainingArguments\n",
        "from transformers import logging\n",
        "logging.set_verbosity_debug()\n",
        "\n",
        "\n",
        "epochs = 2\n",
        "batch_size = 3\n",
        "num_steps = len(train_dataset) * epochs // batch_size\n",
        "warmup_steps = num_steps // 10  # 10% of the training steps\n",
        "save_steps = num_steps // epochs    # Save a checkpoint at the end of each epoch\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = f'{base_dir}/models/{dataset_type}/BERT-pair/{task}/',          \n",
        "    num_train_epochs = epochs,              \n",
        "    per_device_train_batch_size = batch_size,  \n",
        "    per_device_eval_batch_size = batch_size,   \n",
        "    warmup_steps = warmup_steps,   \n",
        "    weight_decay = 0.01,               \n",
        "    logging_dir = f'{base_dir}/logs/{dataset_type}/BERT-pair/{task}/',            \n",
        "    logging_steps = 10,\n",
        "    evaluation_strategy = 'epoch',\n",
        "    learning_rate = 2e-5,\n",
        "    save_steps = save_steps\n",
        ")\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    'xlm-roberta-base',\n",
        "    architectures = ['AutoModelForSequenceClassification'],\n",
        "    hidden_size = 768,\n",
        "    num_hidden_layers = 12,\n",
        "    num_attention_heads = 12,\n",
        "    hidden_dropout_prob = 0.1,\n",
        "    num_labels = num_classes\n",
        ")    \n",
        "\n",
        "#load_finetuned_model = True\n",
        "#if not load_finetuned_model:\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-base\", ignore_mismatched_sizes=True, config=config)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         \n",
        "    args=training_args,                  \n",
        "    train_dataset=train_dataset,         \n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics             \n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(f\"{base_dir}/models/{dataset_type}/BERT-pair/{task}/last_step3\")\n",
        "\n",
        "\n",
        "#else:\n",
        "#    model = BertForSequenceClassification.from_pretrained(f\"{base_dir}/models/{dataset_type}/BERT-pair/{task}/last_step\")\n",
        "#\n",
        "#    trainer = Trainer(\n",
        "#        model=model,                         \n",
        "#        args=training_args,                  \n",
        "#        train_dataset=train_dataset,         \n",
        "#        eval_dataset=val_dataset,\n",
        "#        compute_metrics=compute_metrics             \n",
        "#    )"
      ],
      "metadata": {
        "id": "86zc14yv1Cpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoConfig, Trainer, TrainingArguments\n",
        "from transformers import logging\n",
        "logging.set_verbosity_debug() \n",
        "\n",
        "epochs = 2\n",
        "batch_size = 3\n",
        "num_steps = len(train_dataset) * epochs // batch_size\n",
        "warmup_steps = num_steps // 10  # 10% of the training steps\n",
        "save_steps = num_steps // epochs    # Save a checkpoint at the end of each epoch\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = f'{base_dir}/models/{dataset_type}/BERT-pair/{task}/',          \n",
        "    num_train_epochs = epochs,              \n",
        "    per_device_train_batch_size = batch_size,  \n",
        "    per_device_eval_batch_size = batch_size,   \n",
        "    warmup_steps = warmup_steps,   \n",
        "    weight_decay = 0.01,               \n",
        "    logging_dir = f'{base_dir}/logs/{dataset_type}/BERT-pair/{task}/',            \n",
        "    logging_steps = 10,\n",
        "    evaluation_strategy = 'epoch',\n",
        "    learning_rate = 2e-5,\n",
        "    save_steps = save_steps\n",
        ")\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    'xlm-roberta-base',\n",
        "    architectures = ['AutoModelForSequenceClassification'],\n",
        "    hidden_size = 768,\n",
        "    num_hidden_layers = 12,\n",
        "    num_attention_heads = 12,\n",
        "    hidden_dropout_prob = 0.1,\n",
        "    num_labels = num_classes\n",
        ")   \n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(f\"{base_dir}/models/{dataset_type}/BERT-pair/{task}/last_step3\", ignore_mismatched_sizes=True)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         \n",
        "    args=training_args,                  \n",
        "    train_dataset=train_dataset,         \n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics             \n",
        ")"
      ],
      "metadata": {
        "id": "6UospXd4G2Oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kor-semeval data로 학습된 Xlm-RoBERTa-base 모델 불러오기\n",
        "from transformers import AutoModelForSequenceClassification, AutoConfig, Trainer, TrainingArguments\n",
        "from transformers import logging\n",
        "logging.set_verbosity_debug()\n",
        "\n",
        "\n",
        "epochs = 2\n",
        "batch_size = 24\n",
        "num_steps = len(train_dataset) * epochs // batch_size\n",
        "warmup_steps = num_steps // 10  # 10% of the training steps\n",
        "save_steps = num_steps // epochs    # Save a checkpoint at the end of each epoch\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = f'{base_dir}/models/{dataset_type}/BERT-pair/{task}/',          \n",
        "    num_train_epochs = epochs,              \n",
        "    per_device_train_batch_size = batch_size,  \n",
        "    per_device_eval_batch_size = batch_size,   \n",
        "    warmup_steps = warmup_steps,   \n",
        "    weight_decay = 0.01,               \n",
        "    logging_dir = f'{base_dir}/logs/{dataset_type}/BERT-pair/{task}/',            \n",
        "    logging_steps = 10,\n",
        "    evaluation_strategy = 'epoch',\n",
        "    learning_rate = 2e-5,\n",
        "    save_steps = save_steps\n",
        ")\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    'xlm-roberta-base',\n",
        "    architectures = ['AutoModelForSequenceClassification'],\n",
        "    hidden_size = 768,\n",
        "    num_hidden_layers = 12,\n",
        "    num_attention_heads = 12,\n",
        "    hidden_dropout_prob = 0.1,\n",
        "    num_labels = num_classes\n",
        ")    \n",
        "#kor-semeval data로 학습된 Xlm-RoBERTa-base 모델 불러오기\n",
        "model = AutoModelForSequenceClassification.from_pretrained(f\"{base_dir}/models/{dataset_type}/BERT-pair/{task}/last_step2\", num_labels = 5, ignore_mismatched_sizes=True)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         \n",
        "    args=training_args,                  \n",
        "    train_dataset=train_dataset,         \n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics             \n",
        ")"
      ],
      "metadata": {
        "id": "pLHzZoZdRFs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#메모리 에러뜰 때 캐시삭제\n",
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "KtpPBKd45Y_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_result = trainer.evaluate(test_dataset)\n",
        "print(evaluation_result)"
      ],
      "metadata": {
        "id": "KNvdSkvY1Lsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'eval_P': 0.8177083333333334, 'eval_R': 0.615686274509804, 'eval_F1': 0.7024608501118568, \n",
        "\n",
        "'eval_4-way': 0.6039215686274509, \n",
        "'eval_3-way': 0.6045142296368989, 'eval_binary': 0.6190954773869347"
      ],
      "metadata": {
        "id": "tICT4mI4dKpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_result2 = trainer.evaluate(train_dataset)\n",
        "print(evaluation_result2)"
      ],
      "metadata": {
        "id": "JprIu8_wNt72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#KR3 수도레이블링 결과 저장\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "results = trainer.predict(test_dataset)\n",
        "\n",
        "results\n",
        "\n",
        "scores = [softmax(prediction) for prediction in results.predictions]\n",
        "predicted_labels = [np.argmax(x) for x in scores]\n",
        "pd.DataFrame(predicted_labels).to_csv('KR3_train_pred.csv', index=False)\n",
        "#kakao_pred_tb = pd.read_csv('/content/kakap_pred.csv')\n"
      ],
      "metadata": {
        "id": "lOMX4xZwJLxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "id": "w3Q1VtM2Z6Q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ensemble을 위한 BERT 확률값 저장\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "results = trainer.predict(test_dataset)\n",
        "\n",
        "results\n",
        "\n",
        "scores = [softmax(prediction) for prediction in results.predictions]\n",
        "#predicted_labels = [np.argmax(x) for x in scores]\n",
        "\n",
        "#results = trainer.predict(test_dataset)\n",
        "#results\n",
        "pd.DataFrame(scores).to_csv('BERT_predtb.csv', index=False)\n",
        "BERT_pred_tb = pd.read_csv('/content/BERT_predtb.csv')"
      ],
      "metadata": {
        "id": "Kl-Qbpsw1LxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BERT_pred_tb = pd.read_csv('/content/BERT_predtb.csv')"
      ],
      "metadata": {
        "id": "vVPK0KePLuxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ensemble을 위한 XLM 확률값 저장\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "results2 = trainer.predict(test_dataset)\n",
        "\n",
        "results2\n",
        "\n",
        "scores2 = [softmax(prediction) for prediction in results2.predictions]\n",
        "#predicted_labels = [np.argmax(x) for x in scores]\n",
        "\n",
        "#results = trainer.predict(test_dataset)\n",
        "#results\n",
        "pd.DataFrame(scores2).to_csv('XLM_KR3_predtb.csv', index=False)\n",
        "XLM_pred_tb = pd.read_csv('/content/XLM_KR3_predtb.csv')"
      ],
      "metadata": {
        "id": "Es5oguSZ26zZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XLM_pred_tb = pd.read_csv('/content/XLM_predtb.csv')"
      ],
      "metadata": {
        "id": "BM84-Af3LxLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Averaging 앙상블 함수 \n",
        "#평가지표가 roc-auc, logloss 등일 경우 산술평균, 기하평균, 조화평균, 멱평균(power mean) 사용, 그 중 멱평균\n",
        "#p=1일 때 산술평균, p=0일 때 기하평균, p=-1일 때 조화평균\n",
        "def averaging_ensemble(list_ = [], cols_size = 5, p=1):\n",
        "    \"\"\"\n",
        "    list_ : 클래스별 확률테이블 작성\n",
        "    \"\"\"\n",
        "    list_size = len(list_)\n",
        "    if list_size < 2:\n",
        "        print(\"2개 이상 넣기\")\n",
        "        return\n",
        "    \n",
        "    pred_means = []\n",
        "    for s_ in range(cols_size):\n",
        "        \n",
        "        preds = [v.iloc[:, s_].tolist() for v in list_]\n",
        "            \n",
        "        if p == 0:\n",
        "            pred_mean = gmean(preds, axis=0)\n",
        "        else:\n",
        "            pred_mean = (np.sum(np.array(preds)**p, axis=0) / len(list_))**(1/p)\n",
        "        \n",
        "        pred_means.append(pred_mean)\n",
        "        \n",
        "    return pred_means"
      ],
      "metadata": {
        "id": "jC6maqUS261s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_list = [\n",
        "    # BERT\n",
        "    BERT_pred_tb, # \n",
        "\n",
        "    # XLM ROBERTA\n",
        "    XLM_pred_tb, # \n",
        "\n",
        "]\n",
        "\n",
        "p=1\n",
        "pred_means = averaging_ensemble(list_=submission_list, p=p)"
      ],
      "metadata": {
        "id": "CHXYL59K264V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "pred_table = pd.DataFrame()\n",
        "\n",
        "for i, pred in enumerate(tqdm(pred_means)):\n",
        "    pred_table[i] = pred\n",
        "  \n",
        "\n",
        "\n",
        "pred_table\n",
        "\n",
        "predicted_labels = np.argmax(np.array(pred_table),axis = 1)\n",
        "\n",
        "#sample_sub['label'] = np.argmax(np.array(pred_table),axis = 1)"
      ],
      "metadata": {
        "id": "AOaYcj6ghnRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = pred_table.values.tolist()\n",
        "#list.toArray(scores2)"
      ],
      "metadata": {
        "id": "WyGF7K9DEzak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scores = pred_means\n",
        "\n",
        "\n",
        "def compute_semeval_accuracy(test_labels, predicted_labels, scores, num_classes=4):\n",
        "    count_considered_examples = 0\n",
        "    count_correct_examples = 0\n",
        "    if num_classes == 4:\n",
        "        for i in range(len(test_labels)):\n",
        "            if test_labels[i] == 4:\n",
        "                continue\n",
        "            new_predicted_label = predicted_labels[i]\n",
        "            if new_predicted_label == 4:\n",
        "                new_scores = scores[i].copy()\n",
        "                new_scores[4] = 0\n",
        "                new_predicted_label = np.argmax(new_scores)\n",
        "            if test_labels[i] == new_predicted_label:\n",
        "                count_correct_examples += 1\n",
        "            count_considered_examples += 1\n",
        "        semeval_accuracy = count_correct_examples / count_considered_examples\n",
        "\n",
        "    elif num_classes == 3:\n",
        "        for i in range(len(test_labels)):\n",
        "            if test_labels[i] >= 3:\n",
        "                continue\n",
        "            new_predicted_label = predicted_labels[i]\n",
        "            if new_predicted_label >= 3:\n",
        "                new_scores = scores[i].copy()\n",
        "                new_scores[3] = 0\n",
        "                new_scores[4] = 0\n",
        "                new_predicted_label = np.argmax(new_scores)\n",
        "            if test_labels[i] == new_predicted_label:\n",
        "                count_correct_examples += 1\n",
        "            count_considered_examples += 1\n",
        "        semeval_accuracy = count_correct_examples / count_considered_examples\n",
        "    elif num_classes == 2:\n",
        "        for i in range(len(test_labels)):\n",
        "            if test_labels[i] == 1 or test_labels[i] >= 3:\n",
        "                continue\n",
        "            new_predicted_label = predicted_labels[i]\n",
        "            if new_predicted_label == 1 or new_predicted_label >= 3:\n",
        "                new_scores = scores[i].copy()\n",
        "                new_scores[1] = 0\n",
        "                new_scores[3] = 0\n",
        "                new_scores[4] = 0\n",
        "                new_predicted_label = np.argmax(new_scores)\n",
        "            if test_labels[i] == new_predicted_label:\n",
        "                count_correct_examples += 1\n",
        "            count_considered_examples += 1\n",
        "        semeval_accuracy = count_correct_examples / count_considered_examples\n",
        "    else:\n",
        "        raise ValueError(\"num_classes must be equal to 2, 3, or 4\")\n",
        "    return semeval_accuracy"
      ],
      "metadata": {
        "id": "rc_U2YfGAtLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p, r, f1 = evaluation.compute_semeval_PRF(test_labels, predicted_labels)\n",
        "four_way = evaluation.compute_semeval_accuracy(test_labels, predicted_labels, scores, 4)\n",
        "three_way = evaluation.compute_semeval_accuracy(test_labels, predicted_labels, scores, 3)\n",
        "two_way = evaluation.compute_semeval_accuracy(test_labels, predicted_labels, scores, 2)\n",
        "print(p)\n",
        "print(r)\n",
        "print(f1)\n",
        "print(four_way)\n",
        "print(three_way)\n",
        "print(two_way)"
      ],
      "metadata": {
        "id": "TTZjQV6V27Ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_output = np.insert(scores, 0, predicted_labels, axis=1)\n",
        "df = pd.DataFrame(csv_output)\n",
        "df[0] = df[0].astype(\"int\")\n",
        "if task.endswith(\"B\"):\n",
        "    header = [\"predicted_label\", \"no\", \"yes\"]\n",
        "else:\n",
        "    header = [\"predicted_label\"]\n",
        "    for label in label2id.keys():\n",
        "        header.append(label)\n",
        "df.to_csv(f\"{base_dir}/results/{dataset_type}/BERT-pair/{task}.csv\", index=False, header=header)"
      ],
      "metadata": {
        "id": "HvGN-LBl1OAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "if base_dir not in sys.path:\n",
        "    sys.path.insert(0, f'{base_dir}/')\n",
        "import evaluation\n",
        "evaluation.main(task, dataset_type, f\"{base_dir}/data\", f\"{base_dir}/results\")"
      ],
      "metadata": {
        "id": "lD1PZUKuthG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluation"
      ],
      "metadata": {
        "id": "M3W8dgKMean7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#KR3 전처리\n",
        "import pandas as pd\n",
        "KR3_pair_test = pd.read_csv('/content/KR3_pair_test.csv')\n",
        "KR3_pair_test"
      ],
      "metadata": {
        "id": "yIdMEoMM1P8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KR3_pair_test = KR3_pair_test.sort_values(by=['original_sentence'])\n",
        "pd.DataFrame(KR3_pair_test).to_csv('KR3_pair_test.csv', index=False)"
      ],
      "metadata": {
        "id": "lNU5IB7i1QBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zMjpw6u61QEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "43yDy_HX1QHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "53mKWMK91QK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hWCkf7ub1QMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RxbQlVk81QPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "95-YmhQF1QRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CzAJS0GT1QUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pUvMfIym1QXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "43bKgO9i1QZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "90VCeRw71Qb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose a dataset and a task { run: \"auto\", display-mode: \"form\" }\n",
        "base_dir = \"/gdrive/MyDrive/Machine_Learning\" #@param {type:\"string\"}\n",
        "dataset_type = \"semeval2014\" #@param [\"sentihood\", \"semeval2014\"]\n",
        "task = \"single\" #@param [\"single\"]"
      ],
      "metadata": {
        "id": "NhB1i_9JthPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "\n",
        "if dataset_type == \"sentihood\":\n",
        "    id2label = {0: \"None\", 1: \"Positive\", 2: \"Negative\"}\n",
        "    label2id = {\"None\": 0, \"Positive\": 1, \"Negative\": 2}\n",
        "elif dataset_type == \"semeval2014\":\n",
        "    id2label = {0: \"긍정\", 1: \"중립\", 2: \"부정\", 3: \"대립\", 4: \"없음\"}\n",
        "    label2id = {\"긍정\": 0, \"중립\" : 1, \"부정\" : 2, \"대립\": 3, \"없음\": 4}\n",
        "\n",
        "if dataset_type == \"sentihood\":\n",
        "    num_classes = 3\n",
        "    locations = [\"location_1_\", \"location_2_\"]\n",
        "    aspects = [\"general\", \"price\", \"safety\", \"transit location\"]\n",
        "elif dataset_type == \"semeval2014\":\n",
        "    num_classes = 5\n",
        "    locations = [\"\"]\n",
        "    aspects = [\"분위기\", \"일화\", \"음식\", \"가격\", \"서비스\"]\n",
        "\n",
        "\n",
        "def get_dataset(path):\n",
        "    original_sentences = []\n",
        "    labels = []\n",
        "    data = pd.read_csv(path, header=0, error_bad_lines= False).values.tolist()\n",
        "    for row in data:\n",
        "        original_sentences.append(row[1])\n",
        "        labels.append(row[3])\n",
        "    return original_sentences, labels\n",
        "\n",
        "\n",
        "train_original_sentences = {}\n",
        "train_labels = {}\n",
        "val_original_sentences = {}\n",
        "val_labels = {}\n",
        "test_original_sentences = {}\n",
        "test_labels = {}\n",
        "\n",
        "for location in locations:\n",
        "    train_original_sentences[location] = {}\n",
        "    train_labels[location] = {}\n",
        "    val_original_sentences[location] = {}\n",
        "    val_labels[location] = {}\n",
        "    test_original_sentences[location] = {}\n",
        "    test_labels[location] = {}\n",
        "    for aspect in aspects:\n",
        "        train_original_sentences[location][aspect], train_labels[location][aspect] = get_dataset(f\"{base_dir}/data/{dataset_type}/BERT-single/{location}{aspect}/train.csv\")\n",
        "        if dataset_type == \"sentihood\":\n",
        "            val_original_sentences[location][aspect], val_labels[location][aspect] = get_dataset(f\"{base_dir}/data/{dataset_type}/BERT-single/{location}{aspect}/dev.csv\")\n",
        "        elif dataset_type == \"semeval2014\":\n",
        "            val_original_sentences[location][aspect], val_labels[location][aspect] = get_dataset(f\"{base_dir}/data/{dataset_type}/BERT-single/{location}{aspect}/test.csv\")\n",
        "            test_original_sentences[location][aspect], test_labels[location][aspect] = get_dataset(f\"{base_dir}/data/{dataset_type}/BERT-single/{location}{aspect}/test.csv\")"
      ],
      "metadata": {
        "id": "Sk-WtIuqqe-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BERT용\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "train_encodings = {}\n",
        "val_encodings = {}\n",
        "test_encodings = {}\n",
        "for location in locations:\n",
        "    train_encodings[location] = {}\n",
        "    val_encodings[location] = {}\n",
        "    test_encodings[location] = {}\n",
        "    for aspect in aspects:\n",
        "        train_encodings[location][aspect] = tokenizer(train_original_sentences[location][aspect], truncation=True, padding=True)\n",
        "        val_encodings[location][aspect] = tokenizer(val_original_sentences[location][aspect], truncation=True, padding=True)\n",
        "        test_encodings[location][aspect] = tokenizer(test_original_sentences[location][aspect], truncation=True, padding=True)"
      ],
      "metadata": {
        "id": "E8S-z_o4qfBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#XLM-RoBERTa용\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
        "\n",
        "train_encodings = {}\n",
        "val_encodings = {}\n",
        "test_encodings = {}\n",
        "for location in locations:\n",
        "    train_encodings[location] = {}\n",
        "    val_encodings[location] = {}\n",
        "    test_encodings[location] = {}\n",
        "    for aspect in aspects:\n",
        "        train_encodings[location][aspect] = tokenizer(train_original_sentences[location][aspect], truncation=True, padding=True)\n",
        "        val_encodings[location][aspect] = tokenizer(val_original_sentences[location][aspect], truncation=True, padding=True)\n",
        "        test_encodings[location][aspect] = tokenizer(test_original_sentences[location][aspect], truncation=True, padding=True)"
      ],
      "metadata": {
        "id": "RHZRzyNsSDQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class ABSA_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n",
        "train_dataset = {}\n",
        "val_dataset = {}\n",
        "test_dataset = {}\n",
        "for location in locations:\n",
        "    train_dataset[location] = {}\n",
        "    val_dataset[location] = {}\n",
        "    test_dataset[location] = {}\n",
        "    for aspect in aspects:\n",
        "        train_dataset[location][aspect] = ABSA_Dataset(train_encodings[location][aspect], train_labels[location][aspect])\n",
        "        val_dataset[location][aspect] = ABSA_Dataset(val_encodings[location][aspect], val_labels[location][aspect])\n",
        "        test_dataset[location][aspect] = ABSA_Dataset(test_encodings[location][aspect], test_labels[location][aspect])"
      ],
      "metadata": {
        "id": "OerRjgw7qfFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[location][aspect]"
      ],
      "metadata": {
        "id": "HZtnv69yTXyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BERT용\n",
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments, BertConfig\n",
        "from transformers import logging\n",
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "\n",
        "\n",
        "logging.set_verbosity_debug()\n",
        "\n",
        "\n",
        "epochs = 2\n",
        "batch_size = 24\n",
        "\n",
        "header = [\"predicted_label\"]\n",
        "for label in label2id.keys():\n",
        "    header.append(label)\n",
        "\n",
        "config = BertConfig.from_pretrained(\n",
        "        'bert-base-multilingual-cased',\n",
        "        architectures = ['BertForSequenceClassification'],\n",
        "        hidden_size = 768,\n",
        "        num_hidden_layers = 12,\n",
        "        num_attention_heads = 12,\n",
        "        hidden_dropout_prob = 0.1,\n",
        "        num_labels = num_classes\n",
        "    )    \n",
        "\n",
        "for location in locations:\n",
        "    for aspect in aspects:\n",
        "        num_steps = len(train_dataset[location][aspect]) * epochs // batch_size\n",
        "        warmup_steps = num_steps // 10  # 10% of the training steps\n",
        "        save_steps = num_steps // epochs    # Save a checkpoint at the end of each epoch\n",
        "\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir = f'{base_dir}/models/{dataset_type}/BERT-single/{location}{aspect}/',          \n",
        "            num_train_epochs = epochs,              \n",
        "            per_device_train_batch_size = batch_size,  \n",
        "            per_device_eval_batch_size = batch_size,   \n",
        "            warmup_steps = warmup_steps,   \n",
        "            weight_decay = 0.01,               \n",
        "            logging_dir = f'{base_dir}/logs/{dataset_type}/BERT-single/{location}{aspect}/',            \n",
        "            logging_steps = 10,\n",
        "            evaluation_strategy = 'epoch',\n",
        "            learning_rate = 2e-5,\n",
        "            save_steps = save_steps,\n",
        "            seed=21\n",
        "        )\n",
        "\n",
        "        model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', config=config)\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,                         \n",
        "            args=training_args,                  \n",
        "            train_dataset=train_dataset[location][aspect],         \n",
        "            eval_dataset=val_dataset[location][aspect]             \n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "\n",
        "        model.save_pretrained(f\"{base_dir}/models/{dataset_type}/BERT-single/{location}{aspect}/last_step\")\n",
        "\n",
        "        results = trainer.predict(test_dataset[location][aspect])\n",
        "\n",
        "        scores = [softmax(prediction) for prediction in results.predictions]\n",
        "        predicted_labels = [np.argmax(x) for x in scores]\n",
        "\n",
        "        csv_output = np.insert(scores, 0, predicted_labels, axis=1)\n",
        "        df = pd.DataFrame(csv_output)\n",
        "        df[0] = df[0].astype(\"int\")\n",
        "        df.to_csv(f\"{base_dir}/results/{dataset_type}/BERT-single/{location}{aspect}.csv\", index=False, header=header)\n",
        "\n",
        "        del training_args\n",
        "        del model\n",
        "        del trainer\n",
        "        del results\n",
        "        del scores\n",
        "        del predicted_labels\n",
        "        del csv_output\n",
        "        del df\n",
        "        gc.collect()"
      ],
      "metadata": {
        "id": "1c7F7MQOqfI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#XLM-RoBERTa용\n",
        "from transformers import AutoModelForSequenceClassification, AutoConfig, Trainer, TrainingArguments\n",
        "from transformers import logging\n",
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "\n",
        "\n",
        "logging.set_verbosity_debug()\n",
        "\n",
        "epochs = 2\n",
        "batch_size = 24\n",
        "\n",
        "header = [\"predicted_label\"]\n",
        "for label in label2id.keys():\n",
        "    header.append(label)\n",
        "\n",
        "config = BertConfig.from_pretrained(\n",
        "        'xlm-roberta-base',\n",
        "        architectures = ['AutoModelForSequenceClassification'],\n",
        "        hidden_size = 768,\n",
        "        num_hidden_layers = 12,\n",
        "        num_attention_heads = 12,\n",
        "        hidden_dropout_prob = 0.1,\n",
        "        num_labels = num_classes\n",
        "    )    \n",
        "\n",
        "for location in locations:\n",
        "    for aspect in aspects:\n",
        "        num_steps = len(train_dataset[location][aspect]) * epochs // batch_size\n",
        "        warmup_steps = num_steps // 10  # 10% of the training steps\n",
        "        save_steps = num_steps // epochs    # Save a checkpoint at the end of each epoch\n",
        "\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir = f'{base_dir}/models/{dataset_type}/BERT-single/{location}{aspect}/',          \n",
        "            num_train_epochs = epochs,              \n",
        "            per_device_train_batch_size = batch_size,  \n",
        "            per_device_eval_batch_size = batch_size,   \n",
        "            warmup_steps = warmup_steps,   \n",
        "            weight_decay = 0.01,               \n",
        "            logging_dir = f'{base_dir}/logs/{dataset_type}/BERT-single/{location}{aspect}/',            \n",
        "            logging_steps = 10,\n",
        "            evaluation_strategy = 'epoch',\n",
        "            learning_rate = 2e-5,\n",
        "            save_steps = save_steps,\n",
        "            seed=21\n",
        "        )\n",
        "\n",
        "        model = BertForSequenceClassification.from_pretrained('xlm-roberta-base', config=config)\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,                         \n",
        "            args=training_args,                  \n",
        "            train_dataset=train_dataset[location][aspect],         \n",
        "            eval_dataset=val_dataset[location][aspect]             \n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "\n",
        "        model.save_pretrained(f\"{base_dir}/models/{dataset_type}/BERT-single/{location}{aspect}/last_step\")\n",
        "\n",
        "        results = trainer.predict(test_dataset[location][aspect])\n",
        "\n",
        "        scores = [softmax(prediction) for prediction in results.predictions]\n",
        "        predicted_labels = [np.argmax(x) for x in scores]\n",
        "\n",
        "        csv_output = np.insert(scores, 0, predicted_labels, axis=1)\n",
        "        df = pd.DataFrame(csv_output)\n",
        "        df[0] = df[0].astype(\"int\")\n",
        "        df.to_csv(f\"{base_dir}/results/{dataset_type}/BERT-single/{location}{aspect}.csv\", index=False, header=header)\n",
        "\n",
        "        del training_args\n",
        "        del model\n",
        "        del trainer\n",
        "        del results\n",
        "        del scores\n",
        "        del predicted_labels\n",
        "        del csv_output\n",
        "        del df\n",
        "        gc.collect()"
      ],
      "metadata": {
        "id": "7B0ZczUmS3ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "if base_dir not in sys.path:\n",
        "    sys.path.insert(0, f'{base_dir}/')\n",
        "import evaluation\n",
        "\n",
        "\n",
        "evaluation.main(task, dataset_type, f\"{base_dir}/data\", f\"{base_dir}/results\")"
      ],
      "metadata": {
        "id": "GzO6S8LiqfMl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}